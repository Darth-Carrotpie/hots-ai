{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install fastai==0.7.0 --no-deps\n!pip install scikit-learn==0.21.3\n# fastai depends also on an older version of torch\n!pip install torch==0.4.1 torchvision==0.1.9\n!pip install torchtext==0.2.3\n!pip show fastai","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from fastai.imports import *\nfrom fastai.torch_imports import *\nfrom fastai.io import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = 'data/mnist/'\nimport os\nos.makedirs(path, exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"URL='https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/' \nFILENAME='mnist.pkl.gz'\nimport gzip\n\n    \ndef load_mnist(filename):\n    print(filename)\n    return pickle.load(gzip.open(filename), encoding='latin-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_data(URL+FILENAME, path+FILENAME)\n((x, y), (x_valid, y_valid), _) = load_mnist(path+FILENAME)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(x), x.shape, type(y), y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalize"},{"metadata":{"trusted":true},"cell_type":"code","source":"mean = x.mean()\nstd = x.std()\nx=(x-mean)/std\nmean, std, x.mean(), x.std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_valid = (x_valid - mean)/std\nx_valid.mean(), x_valid.std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Helper methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show(img, title=None):\n    plt.imshow(img, cmap='gray');\n    if title is not None: plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plots(ims, figsize=(12,6), rows=2, titles=None):\n    f = plt.figure(figsize = figsize)\n    cols = len(ims)//rows\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None: sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"?plt.imshow","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_imgs = np.reshape(x_valid, (-1,28,28)); x_imgs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show(x_imgs[0,10:15,10:15])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plots(x_imgs[:8], titles=y_valid[:8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Neural Net for Logistic Regression in PyTorch"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.metrics import *\n\n\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.model import *\nfrom fastai.dataset import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so this error is due to a bad python version, in 3.7 async is now a keyword..., need 3.6 python version to run this old fastai"},{"metadata":{"trusted":true},"cell_type":"code","source":"net = nn.Sequential(\n    nn.Linear(28*28, 10),\n    nn.LogSoftmax()\n).cuda()\n#this is a two layer neural net\n#.cuda tells to run on GPU, if didnt say that, would run on CPU","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mp = ImageClasifireData.from_arrays(path, (x, y), (x_valid, y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nprint(sys.version)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = nn.NLLLoss() #negative low likelyhood loss = cross entropy. Either binary or categorical\nmetrics=[accuracy]\nopt=optim.Adam(net.parameters())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fitting the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"fit(net, md, epochs=1, crit=loss, opt=opt, metrics=metrics) #pythorch uses word criterion instead of loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = predict(net, md.val_dl)\npreds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.argmax(axis=1)[:5] #argmax > figures out on this axis (for 10 in this case), return the index of the max value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = preds.argmax(1) #saving this","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(preds == y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plots(x_imgs[:8], titles=preds[:8])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#we have just built a logistic regression, it is not a deep neural net yet"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_weights(dims): return nn.Parameter(torch.randn(dims)/dims[0]) # because there are many layers, so need to make sure that the mean inputs not gonna change, otherwise its gonna diverge infinitely or converge it the number is bigger (gradient explosion)\n#google: kaiming he initialization","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class LogReg(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1_w = get_weights(28*28, 10) # Layer 1 weights\n        self.l1_b = get_weights(10)   # Layer 1 bias\n    def forward(self, x): #in pytorch it has special meaning, it will get called when the layer gets calculated, its gonna get passed data form previous layer\n        x = x.view(x.size(0), -1) #view = reshape\n        x = torch.matmul(x, self.l1_w) +self.l1_b # Linear layer\n        x = torch.log(torch.exp(x)/(torch.exp(x).sum(dim=0))) #this is a softmax (because we want all probabiliteis of outcomes to sumup to 1), so it behaves so tthat it returns something that behaves as probabilities\n        return x\n#there is a lot of info on pytorch website on how to create tensors and modify stuff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we define a * x, which are weights, then\n#+b is called bias","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net2 = LogReg().cuda()\nopt=optim.Adam(net2.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fit(net2, md, epochs=1, crit=loss, opt=opt, metrics=metrics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"1:19:30","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}